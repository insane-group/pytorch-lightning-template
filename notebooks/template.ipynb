{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Getting started with PyTorch Lightning\n",
    "\n",
    "In this notebook, weâ€™ll go over the basics of lightning by preparing models to train on the [MNIST Handwritten Digits dataset](https://en.wikipedia.org/wiki/MNIST_database).\n",
    "\n",
    "> This notebook is heavily inspired by the original material provided by lightning.ai on [Introduction to PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/mnist-hello-world.html) and [Data Modules](https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/datamodules.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from lightning import LightningModule\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from lightning.pytorch.loggers import CSVLogger, TensorBoardLogger\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import MaxMetric, MeanMetric\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
    "\n",
    "BASE_DIR = Path().cwd().resolve().parent\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "LOG_DIR = BASE_DIR / \"logs\"\n",
    "MNIST_DIR = BASE_DIR / \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataModules` decouple data-related hooks from `LightningModule`, enabling dataset-agnostic models.\n",
    "\n",
    "- `__init__`: Defines `data_dir`, a common transform for all splits, and `self.dims`.  \n",
    "- `prepare_data`: Downloads the dataset (if needed) without making state assignments.  \n",
    "- `setup`: Loads data, prepares PyTorch datasets, and handles logic for â€˜fitâ€™ and â€˜testâ€™ stages. Runs safely across GPUs.  \n",
    "- Dataloaders: `train_dataloader()`, `val_dataloader()`, and `test_dataloader()` return `DataLoader` instances wrapping the prepared datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path,\n",
    "        num_workers: int = 1,\n",
    "        pin_memory: bool = True,\n",
    "        batch_size: int = 16,\n",
    "        transforms: list | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.dims = (1, 28, 28)\n",
    "        self.num_classes = 10\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.batch_size = batch_size\n",
    "        self.transforms = transforms or []\n",
    "        self._default_transforms = [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            transform = T.Compose(self.transforms + self._default_transforms)\n",
    "\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            transform = T.Compose(self._default_transforms)\n",
    "\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_train,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            shuffle=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_val,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_test,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLitModule(LightningModule):\n",
    "    \"\"\"Example of a `LightningModule` for MNIST classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "        input_size: int = 784,\n",
    "        lin1_size: int = 256,\n",
    "        lin2_size: int = 256,\n",
    "        lin3_size: int = 256,\n",
    "        output_size: int = 10,\n",
    "        compile: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the MNISTLitModule.\n",
    "\n",
    "        Args:\n",
    "            optimizer (torch.optim.Optimizer): The optimizer to use.\n",
    "            scheduler (torch.optim.lr_scheduler._LRScheduler): The learning rate scheduler to use.\n",
    "            input_size (int, optional): The size of the input layer. Defaults to 784.\n",
    "            lin1_size (int, optional): The size of the first linear layer. Defaults to 256.\n",
    "            lin2_size (int, optional): The size of the second linear layer. Defaults to 256.\n",
    "            lin3_size (int, optional): The size of the third linear layer. Defaults to 256.\n",
    "            output_size (int, optional): The size of the output layer. Defaults to 10.\n",
    "            compile (bool, optional): Whether to compile the module or not. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # this line allows to access init params with 'self.hparams' attribute\n",
    "        # also ensures init params will be stored in ckpt\n",
    "        self.save_hyperparameters(logger=False, ignore=[\"net\"])\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, lin1_size),\n",
    "            nn.BatchNorm1d(lin1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(lin1_size, lin2_size),\n",
    "            nn.BatchNorm1d(lin2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(lin2_size, lin3_size),\n",
    "            nn.BatchNorm1d(lin3_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(lin3_size, output_size),\n",
    "        )\n",
    "\n",
    "        # loss function\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # metric objects for calculating and averaging accuracy across batches\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "        # for averaging loss across batches\n",
    "        self.train_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "        self.test_loss = MeanMetric()\n",
    "\n",
    "        # for tracking best so far validation accuracy\n",
    "        self.val_acc_best = MaxMetric()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Performs a forward pass through the model `self.net`.\n",
    "\n",
    "        Args:\n",
    "            x: A tensor of images.\n",
    "\n",
    "        Returns:\n",
    "            A tensor of logits.\n",
    "        \"\"\"\n",
    "        batch_size, _, _, _ = x.size()\n",
    "\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        return self.net(x)\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        \"\"\"Lightning hook that is called when training begins.\"\"\"\n",
    "        # by default lightning executes validation step sanity checks before training starts,\n",
    "        # so it's worth to make sure validation metrics don't store results from these checks\n",
    "        self.val_loss.reset()\n",
    "        self.val_acc.reset()\n",
    "        self.val_acc_best.reset()\n",
    "\n",
    "    def model_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Performs a single model step on a batch of data.\n",
    "\n",
    "        Args:\n",
    "            batch: A batch of data (a tuple) containing the input tensor of images and target\n",
    "                labels.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing (in order):\n",
    "                - A tensor of losses.\n",
    "                - A tensor of predictions.\n",
    "                - A tensor of target labels.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return loss, preds, y\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Perform a single training step on a batch of data from the training set.\n",
    "\n",
    "        Args:\n",
    "            batch (Tuple[torch.Tensor, torch.Tensor]): A batch of data containing the input tensor\n",
    "                of images and target labels.\n",
    "            batch_idx (int): The index of the current batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of losses between model predictions and targets.\n",
    "        \"\"\"\n",
    "        loss, preds, targets = self.model_step(batch)\n",
    "\n",
    "        # update and log metrics\n",
    "        self.train_loss(loss)\n",
    "        self.train_acc(preds, targets)\n",
    "        self.log(\n",
    "            \"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"train/acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "\n",
    "        # return loss or backpropagation will fail\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        \"Lightning hook that is called when a training epoch ends.\"\n",
    "        pass\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> None:\n",
    "        \"\"\"Perform a single validation step on a batch of data from the validation set.\n",
    "\n",
    "        Args:\n",
    "            batch: A tuple containing the input tensor of images and target labels.\n",
    "            batch_idx: The index of the current batch.\n",
    "        \"\"\"\n",
    "        loss, preds, targets = self.model_step(batch)\n",
    "\n",
    "        # update and log metrics\n",
    "        self.val_loss(loss)\n",
    "        self.val_acc(preds, targets)\n",
    "        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        \"Lightning hook that is called when a validation epoch ends.\"\n",
    "        acc = self.val_acc.compute()  # get current val acc\n",
    "        self.val_acc_best(acc)  # update best so far val acc\n",
    "        # log `val_acc_best` as a value through `.compute()` method, instead of as a metric object\n",
    "        # otherwise metric would be reset by lightning after each epoch\n",
    "        self.log(\n",
    "            \"val/acc_best\", self.val_acc_best.compute(), sync_dist=True, prog_bar=True\n",
    "        )\n",
    "\n",
    "    def test_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> None:\n",
    "        \"\"\"Performs a single test step on a batch of data from the test set.\n",
    "\n",
    "        Args:\n",
    "            batch (Tuple[torch.Tensor, torch.Tensor]): A batch of data containing the input tensor\n",
    "                of images and target labels.\n",
    "            batch_idx (int): The index of the current batch.\n",
    "        \"\"\"\n",
    "        loss, preds, targets = self.model_step(batch)\n",
    "\n",
    "        # update and log metrics\n",
    "        self.test_loss(loss)\n",
    "        self.test_acc(preds, targets)\n",
    "        self.log(\n",
    "            \"test/loss\", self.test_loss, on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "        self.log(\"test/acc\", self.test_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        \"\"\"Lightning hook that is called when a test epoch ends.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        \"\"\"Called at the beginning of fit (train + validate), validate, test, or predict.\n",
    "\n",
    "        This is a good place to build models dynamically or adjust something about them. This\n",
    "        hook is called on every process when using DDP.\n",
    "\n",
    "        Args:\n",
    "            stage: One of \"fit\", \"validate\", \"test\", or \"predict\".\n",
    "        \"\"\"\n",
    "        if self.hparams.compile and stage == \"fit\":\n",
    "            self.net = torch.compile(self.net)\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        \"\"\"Configure optimizers and learning-rate schedulers.\n",
    "\n",
    "        Returns:\n",
    "            A dict containing the configured optimizers and learning-rate schedulers to be used for training.\n",
    "        \"\"\"\n",
    "        optimizer = self.hparams.optimizer(params=self.trainer.model.parameters())\n",
    "        if self.hparams.scheduler is not None:\n",
    "            scheduler = self.hparams.scheduler(optimizer=optimizer)\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": \"val/loss\",\n",
    "                    \"interval\": \"epoch\",\n",
    "                    \"frequency\": 1,\n",
    "                },\n",
    "            }\n",
    "        return {\"optimizer\": optimizer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we initialize and train the LitModel using the MNISTDataModuleâ€™s configuration settings and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(save_dir=LOG_DIR, name=\"tmp\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    num_nodes=1,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(seed=42, workers=True)\n",
    "loggers = [\n",
    "    CSVLogger(LOG_DIR, name=\"csv\"),\n",
    "    TensorBoardLogger(LOG_DIR, name=\"tensorboard\", log_graph=True),\n",
    "]\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=MODEL_DIR,\n",
    "        filename=\"{epoch}_{val/loss:.2f}_{val_accuracy:.2f}\",\n",
    "        save_top_k=10,\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val/loss\", min_delta=2e-4, patience=8, verbose=False, mode=\"min\"\n",
    "    ),\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "]\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    fast_dev_run=False,\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    num_nodes=1,\n",
    "    logger=loggers,\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=10,\n",
    "    min_epochs=5,\n",
    "    overfit_batches=0.0,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "transforms = [\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.RandomRotation(degrees=45),\n",
    "]\n",
    "\n",
    "datamodule = MNISTDataModule(MNIST_DIR, batch_size=1024, transforms=transforms)\n",
    "model = MNISTLitModule(\n",
    "    optimizer=torch.optim.Adam,\n",
    "    scheduler=partial(torch.optim.lr_scheduler.StepLR, step_size=1, gamma=0.1),\n",
    "    compile=False,\n",
    ")\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir={LOG_DIR}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOzXqA8/1Y3Kw5skeArTGRA",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "pl-mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
